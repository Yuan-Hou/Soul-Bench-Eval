# Video-Eval User Guide

This project provides an extensible objective evaluation framework for video generation models. Through a unified entry script `evaluate.py`, multiple evaluation subjects can be executed in series on the same batch of generation results, outputting a structured JSON report. The following sections detail the project structure, preparation, overall process, and usage points for each evaluation subject.

## Directory Structure Overview

```
Video-Eval/
├── evaluate.py           # Main evaluation script, loads data and runs subjects sequentially
├── utils.py              # Video/image I/O and common utility functions
├── video.py              # `VideoData` data structure definition
├── docs/                 # Documentation directory
│   ├── av_align_usage.md # AV-Align evaluation guide
│   ├── parallel_evaluation_guide.md
│   └── qwen_vl_vllm_usage.md
└── subjects/             # Concrete evaluation subject implementations
    ├── arcface_consistency.py
    ├── av_align.py       # Audio-Video Alignment evaluation
    ├── latent_sync.py
    ├── qwen_vl_vllm.py
    └── video_quality.py
```

Core Process:

1. `evaluate.py` constructs a list of `VideoData` based on the specified input/output directories.
2. Imports modules from the `subjects/` directory based on the `--evaluate_subjects` argument and calls their `evaluate` functions.
3. Each subject writes evaluation results to `VideoData` (`register_result`).
4. All results are finally saved as `evaluation_results.json`.

The `VideoData` object automatically associates reference images (`.png`), audio (`.wav`), and text (`.txt`) with the same name as the video, and provides convenient methods like `has_image/has_audio/has_text` for subjects to call.

## Environment Preparation

- Python 3.10 and above (recommended version compatible with dependencies).
- Common dependencies (can be installed via `pip install -r requirements.txt`):
  - `torch`, `torchvision`
  - `transformers`, `tqdm`, `numpy`
  - `decord`, `opencv-python`, `pillow`
- Each subject may require additional third-party libraries or model weights, see below for details.

## Quick Start

1. **Prepare Evaluation Data**
   - `--model_input_dir`: Folder containing reference materials, including `.png` (reference image), `.wav` (reference audio), `.txt` (text prompt) with the same name as the video. Can be omitted if not needed.
   - `--model_output_dir`: Video results generated by the model to be evaluated, extension must be `.mp4`.

2. **Select Evaluation Subjects**
   - Separated by commas, e.g., `--evaluate_subjects arcface_consistency,av_align`
   - `--model_args` can provide JSON strings for each subject in the same quantity to pass additional configurations.

3. **Run Example**

   ```bash
   python evaluate.py \
     --model_input_dir /path/to/inputs \
     --model_output_dir /path/to/outputs \
     --results_dir ./evaluation_results \
     --evaluate_subjects arcface_consistency,av_align \
     --model_args '{}','{}'
   ```

   Common arguments:
   - `--device`: Default `cuda`, some subjects support `cpu`.
   - `--batch_size`: Batch size for processing frames, default `16`.
   - `--sampling`: Number of frames to sample per video. Default samples 16 frames; change `DEFAULT_ALL_FRAMES` to `True` to use all frames by default.

4. **View Results**
   - Results for all `VideoData` will be written to `results_dir/evaluation_results.json`.
   - Each entry in the JSON contains the video path and the evaluation results for each subject.

## Evaluation Subjects Details

### 1. Identity Consistence (ArcFace)

- **Function**: Uses the InsightFace ArcFace model to measure the similarity between faces in the video and the reference image face.
- **Dependencies**: `insightface`, `opencv-python`, `torch` (GPU support required for faster detection).
- **Key Parameters**:
  - `name`: InsightFace model combination, default `buffalo_l`.
  - `det_size`: Face detection input size, default `(640, 640)`.
- **Usage Requirements**:
  - Must provide the corresponding reference face image (`.png`).
  - If no face is detected, an exception will be thrown or the frame will be skipped.
  - Results include `arcface_consistency` mean and frame-by-frame similarity list.
- **Usage Example**:
  ```bash
  python evaluate.py \
    --model_input_dir /path/to/inputs \
    --model_output_dir /path/to/outputs \
    --evaluate_subjects arcface_consistency \
    --model_args '{"name": "buffalo_l", "det_size": [640, 640]}'
  ```

### 2. LSE-D/LSE-C (Latent Sync)

- **Function**: Integrates ByteDance's open-source LatentSync project to evaluate lip-sync and speech synchronization.
- **Dependencies**:
  - System requires `git`, `ffmpeg` (LatentSync runtime requirements).
  - Python packages: `huggingface_hub` (auto download weights), and LatentSync's own dependencies (install according to its README after cloning).
- **Key Parameters**:
  - `repo_dir`: LatentSync code location, default `third_party/LatentSync`.
  - `force_clone`: Force re-clone if `true`.
  - `min_track`: Minimum number of frames for face tracking, default `50`.
  - `syncnet_checkpoint`, `huggingface_repo_id`: Custom weight path or source.
  - `subject_name`: Custom key name for writing results, default `latent_sync`.
- **Output**: Each video contains fields like `confidence`, `av_offset`, `num_crops`. Records `error` if failed.
- **Usage Example**:
  ```bash
  python evaluate.py \
    --model_input_dir /path/to/inputs \
    --model_output_dir /path/to/outputs \
    --evaluate_subjects latent_sync \
    --model_args '{"min_track": 50}'
  ```

### 3. Audio-Video Alignment (AV-Align)

- **Function**: Uses the AV-Align metric to evaluate the alignment between audio and video modalities. By detecting audio peaks and video peaks (based on optical flow), it calculates their Intersection over Union (IoU) to quantify synchronization. Higher IoU scores indicate better alignment.
- **Dependencies**:
  - Python packages: `opencv-python` (`cv2`), `librosa`, `numpy`.
  - System tools: `ffmpeg` (used to extract audio from video if not provided separately).
- **Key Parameters** (passed via `--model_args` JSON):
  - `subject_name`: Custom key name for writing results, default `av_align`.
  - `downsample`: Frame downsampling factor, default `2`. Used to accelerate calculation:
    - `1`: No downsampling (slowest, most accurate)
    - `2`: Process every other frame (2x speed, recommended)
    - `4`: Process every 4th frame (4x speed)
- **Input Requirements**:
  - Video file (`.mp4`) required.
  - If corresponding audio file (`.wav`) is provided, it will be used directly; otherwise, audio will be extracted from the video automatically.
- **Output Fields**:
  - `iou_score`: Audio-video alignment IoU score (between 0-1, higher is better).
  - `num_audio_peaks`: Number of detected audio peaks.
  - `num_video_peaks`: Number of detected video peaks.
  - `fps`: Effective frame rate of the video (considering downsampling).
  - `downsample_factor`: Actual downsampling factor used.
  - Records `error` field if evaluation fails.
- **Performance Optimization**:
  - ✅ Vectorized calculation: Use NumPy arrays instead of Python lists
  - ✅ Direct grayscale extraction: Avoid repeated BGR->Grayscale conversion
  - ✅ Pre-allocated memory: Reduce dynamic expansion overhead
  - ✅ Frame downsampling: Optional frame skipping for acceleration (default 2x)
  - ✅ Optimized peak detection: Vectorized local maxima finding
- **Technical Details**:
  - Audio peak detection: Uses Onset Detection algorithm (librosa).
  - Video peak detection: Uses Farneback optical flow to calculate inter-frame motion, local maxima are video peaks (filtering out static scenes with magnitude < 0.1).
  - IoU calculation: Each video peak matches at most one audio peak, matching window is ±1 frame.
- **Usage Example**:
  ```bash
  # Default settings (2x speed)
  python evaluate.py \
    --model_input_dir /path/to/inputs \
    --model_output_dir /path/to/outputs \
    --evaluate_subjects av_align \
    --model_args '{}'
  
  # Highest precision (no downsampling, slower)
  python evaluate.py \
    --model_input_dir /path/to/inputs \
    --model_output_dir /path/to/outputs \
    --evaluate_subjects av_align \
    --model_args '{"downsample": 1}'
  
  # Fast mode (4x speed)
  python evaluate.py \
    --model_input_dir /path/to/inputs \
    --model_output_dir /path/to/outputs \
    --evaluate_subjects av_align \
    --model_args '{"downsample": 4}'
  ```

### 4. Video-Text Consistence (Qwen-VL)

- **Function**: Calls multimodal large models (default Qwen3-VL) via vLLM to read videos and answer custom prompt questions, recording model output.
- **Dependencies**: `vllm` and its hardware dependencies (CUDA environment, corresponding VRAM requirements).
- **Key Parameters**:
  - `model_name`: Default `Qwen/Qwen3-VL-2B-Thinking`.
  - `prompt_template` or `prompt_template_path`: Template for generating text prompts, supports `str.format` placeholders.
  - `template_variables`: Optional default variables for the template.
  - `system_prompt`: Additional system message.
  - `include_video`: Whether to pass video to the model, default `true`.
  - `sampling_options`: Settings passed to `SamplingParams`, e.g., `{"max_tokens":512,"temperature":0.0}`.
  - `engine_options` / other pass-through parameters: Configuration passed to `vllm.LLM`, such as `tensor_parallel_size`, `dtype`, etc.
  - `store_messages`: If `true`, additionally saves complete messages and raw output.
- **Input Requirements**: Video paths must be visible to vLLM (script defaults to allowing local file paths `file://`).
- **Usage Example**:
  ```bash
  python evaluate.py \
    --model_input_dir /path/to/inputs \
    --model_output_dir /path/to/outputs \
    --evaluate_subjects qwen_vl_vllm \
    --model_args '{"model_name": "Qwen/Qwen2-VL-2B-Instruct", "prompt_template": "Describe this video."}'
  ```

### 5. Video Quality (FineVQ)

- **Function**: Automatically calls the FineVQ project to score video visual quality.
- **Dependencies**:
  - System requires `git`, `ffmpeg` (FineVQ dependency environment).
  - Python packages: Dependencies in the FineVQ repository will be installed on demand at runtime.
- **Key Parameters**:
  - `repo_dir`: FineVQ repository storage path, default `third_party/FineVQ`.
  - `repo_url`: Repository address, default official repository.
  - `install_dependencies`: Whether to install dependencies on first run, default `true`.
  - `force_clone`: Force re-clone repository.
  - `per_device_batch_size`, `batch_size`, `nproc_per_node`: Control batch size and process count for distributed inference.
  - `use_bf16`: Whether to enable bfloat16, default `true`.
  - `env`: Additional environment variables (e.g., `CUDA_VISIBLE_DEVICES`).
  - `extra_args`: Append or override FineVQ CLI arguments as a list.
- **Execution Flow**:
  1. Symlink target videos to a temporary directory and generate `meta.json` required by FineVQ.
  2. Call FineVQ official `torch.distributed.run` inference script.
  3. Parse output CSV and metric files, and write to `video_quality` result field.
- **Output Fields**: `video_quality_score` (usually corresponds to `pred_score`), detailed raw metrics, and optional overall metrics `aggregate_metrics`.
- **Usage Example**:
  ```bash
  python evaluate.py \
    --model_input_dir /path/to/inputs \
    --model_output_dir /path/to/outputs \
    --evaluate_subjects video_quality \
    --model_args '{"use_bf16": true}'
  ```

## Custom Extension

- To add a new evaluation subject, create a Python file with the same name in `subjects/` and export the `evaluate(data_list, device, batch_size, sampling, model_args)` function.
- Custom results for each `VideoData` are written via `register_result(subject_name, payload)` and will eventually appear in the summary JSON.

## FAQ

- **High Memory Usage**: Adjust `--sampling` to reduce the number of sampled frames per video, or modify `DEFAULT_ALL_FRAMES=False`.
- **Missing Dependencies or Models**: Install required Python packages or download weights according to error messages, manually preparing the `checkpoints/` directory if necessary.
- **Evaluation Order**: `evaluate.py` executes in the order specified in `--evaluate_subjects`. Changes to `data_list` by one subject are passed to the next.

Hope this guide helps you quickly understand and use the Video-Eval framework. For further customization or troubleshooting, please refer to the source code of the corresponding subject and adjust according to project needs.
