#!/usr/bin/env python3
"""
æµ‹è¯• qwen_vl_vllm æ¨¡å—çš„ OpenAI API æ”¯æŒ

ä½¿ç”¨æ–¹æ³•ï¼š
1. å…ˆå¯åŠ¨ vLLM OpenAI å…¼å®¹æœåŠ¡å™¨ï¼š
   python -m vllm.entrypoints.openai.api_server \
       --model Qwen/Qwen3-VL-2B-Thinking \
       --port 8000

2. è¿è¡Œæ­¤æµ‹è¯•è„šæœ¬ï¼š
   python test_openai_api.py
"""

import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

def test_imports():
    """æµ‹è¯•å¯¼å…¥"""
    print("æµ‹è¯•å¯¼å…¥æ¨¡å—...")
    try:
        from subjects import qwen_vl_vllm
        print("âœ“ æˆåŠŸå¯¼å…¥ qwen_vl_vllm æ¨¡å—")
        return True
    except Exception as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_openai_available():
    """æµ‹è¯• OpenAI å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
    print("\næµ‹è¯• OpenAI å®¢æˆ·ç«¯...")
    try:
        from openai import OpenAI
        print("âœ“ OpenAI å®¢æˆ·ç«¯å¯ç”¨")
        return True
    except ImportError:
        print("âœ— OpenAI å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install openai")
        return False

def test_vllm_available():
    """æµ‹è¯• vLLM æ˜¯å¦å¯ç”¨"""
    print("\næµ‹è¯• vLLM...")
    try:
        from vllm import LLM, SamplingParams
        print("âœ“ vLLM å¯ç”¨")
        return True
    except ImportError:
        print("âœ— vLLM ä¸å¯ç”¨ï¼ˆä»…åœ¨ä½¿ç”¨æœ¬åœ°æ¨ç†æ—¶éœ€è¦ï¼‰")
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\næµ‹è¯•é…ç½®åŠ è½½...")
    from subjects.qwen_vl_vllm import _load_template, _build_sampling_params
    
    # æµ‹è¯•åŸºæœ¬æ¨¡æ¿åŠ è½½
    model_args = {
        "prompt_template": "Test prompt: {video_text}",
        "template_variables": {"default_var": "test"}
    }
    
    try:
        template = _load_template(model_args)
        result = template.render({"video_text": "hello"})
        assert "hello" in result
        print("âœ“ æ¨¡æ¿åŠ è½½å’Œæ¸²æŸ“æˆåŠŸ")
    except Exception as e:
        print(f"âœ— æ¨¡æ¿åŠ è½½å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•é‡‡æ ·å‚æ•°
    try:
        sampling_opts = {"max_tokens": 256, "temperature": 0.5}
        params = _build_sampling_params(sampling_opts)
        assert params["max_tokens"] == 256
        assert params["temperature"] == 0.5
        print("âœ“ é‡‡æ ·å‚æ•°æ„å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âœ— é‡‡æ ·å‚æ•°æ„å»ºå¤±è´¥: {e}")
        return False
    
    return True

def test_openai_connection(api_base="http://localhost:8000/v1"):
    """æµ‹è¯• OpenAI API è¿æ¥"""
    print(f"\næµ‹è¯• OpenAI API è¿æ¥ ({api_base})...")
    try:
        from openai import OpenAI
        client = OpenAI(base_url=api_base, api_key="EMPTY")
        
        # å°è¯•åˆ—å‡ºæ¨¡å‹
        models = client.models.list()
        print(f"âœ“ æˆåŠŸè¿æ¥åˆ° API æœåŠ¡å™¨")
        print(f"  å¯ç”¨æ¨¡å‹: {[m.id for m in models.data]}")
        return True
    except Exception as e:
        print(f"âœ— è¿æ¥å¤±è´¥: {e}")
        print("  è¯·ç¡®ä¿ vLLM æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
        return False

def main():
    print("=" * 60)
    print("Qwen3-VL vLLM æ¨¡å—æµ‹è¯•")
    print("=" * 60)
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    results.append(("æ¨¡å—å¯¼å…¥", test_imports()))
    results.append(("OpenAI å®¢æˆ·ç«¯", test_openai_available()))
    results.append(("vLLM", test_vllm_available()))
    results.append(("é…ç½®åŠ è½½", test_config_loading()))
    results.append(("API è¿æ¥", test_openai_connection()))
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    for name, passed in results:
        status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
        print(f"{name:20s}: {status}")
    
    total = len(results)
    passed = sum(1 for _, p in results if p)
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
        return 1

if __name__ == "__main__":
    sys.exit(main())
